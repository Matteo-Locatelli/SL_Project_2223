\subsection{Ridge regression}

Now we applied the ridge regression to perform variable selection. In \Fig~\ref{fig:RidgeCoefVsLambda} is shown the coefficients' behavior as $\lambda$ increases. 

It is possible to observe that for enough large values of $\lambda$, around $10^2$, all the coefficients tend to become close to zero. However, they are never exactly zero.

To select the optimal value of the penalization factor $\lambda$ we employed \textit{k}-fold cross-validation with $\textit{k}=10$. In \Fig~\ref{fig:RidgeCvPlot} it is possible to observe the trend of the cross-validated mean square error (MSE) when $\lambda$ increases. The optimal value was found to be $\lambda_{opt} = 0.0268$.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{ImageFiles/Regression/Ridge/RidgeCoefVsLambda.pdf}
		\caption{}
		\label{fig:RidgeCoefVsLambda}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{ImageFiles/Regression/Ridge/RidgeCvPlot.pdf}
		\caption{}
		\label{fig:RidgeCvPlot}
	\end{subfigure}
	\caption{Ridge cv. (a) Coeficients as function of $\lambda$. (b) CV MSE as function of lambda.}
	\label{fig:FinalFSSM}
\end{figure}

We fit the final model using the optimal $\lambda$ value ($\lambda_{opt}$). The resulting coefficients are displayed in \Tab~\ref{table:FinalRidgeCoef}. For this model we computed also the test MSE and it is $MSE_{test} = 1.2577$.

In this case the ridge penalization does not reduce the number of coefficients, and it does not even improve significantly the MSE. We then can conclude that using ridge regression is not useful for our purposes.
