\subsection{Support vector machines}

As a last method, we tried applying the support vector machine (SVM) technique, with three different kernel function, in combination with cross-validation to see if we can achieve better performances. The results are displayed in \Tab~\ref{table:SVMkernels}.

\begin{table}[H]
	\centering
	\begin{tabular}{|| l | r ||} 
		\hline
		Kernel & Accuracy \\
		\hline
		Linear & $68.75\%$ \\
		\hline
		Polynomial & $67.81\%$ \\
		\hline
		Radial & $67.50\%$ \\
		\hline
	\end{tabular}
	\caption{SVM kernel.}
	\label{table:SVMkernels}
\end{table} 

Unfortunately, even the SVM technique did not improve the classification performances in this case. 

\begin{center}
	
	\labelText{
		\texttt{svm\_cross <- tune (svm, y$\sim$., data=dataframe\_train, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))}	
	}{mod:svmcross}
	
	\labelText{
		\texttt{svm\_rad\_tune <- tune(svm, y$\sim$., data=dataframe\_train, kernel="radial", ranges=list(cost=c(0.01, 0.1, 1, 10, 100), gamma=c(0.5, 1, 2, 3, 4)))}	
	}{mod:svmrad}
	
	\labelText{	
		\texttt{svm\_pol\_tune <- tune(svm, y$\sim$., data=dataframe\_train, kernel="polynomial", ranges=list(cost=c(0.01, 0.1, 1, 10, 100), degree=c(0.5, 1, 2, 3, 4)))}	
	}{mod:svmpol}
	
\end{center}