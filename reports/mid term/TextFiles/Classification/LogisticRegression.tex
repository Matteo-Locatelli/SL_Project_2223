\subsection{Logistic regression}\label{appendix:lr}

As first approach we tried to model our target using logistic regression.

First of all we fitted a model using all the features, that we will name model A (\Mod~\ref{mod:lrall}), and the result is displayed in \Tab~\ref{table:LRAllSum}. The model suggested that many variables are not significantly different from zero, with respect to an $\alpha = 0.01$. Therefore we removed those variables and fitted the model again getting the one in \Tab~\ref{table:LRImpSum}, that we will name model B (\Mod~\ref{mod:lrimp}).

\begin{center}
	\labelText{
		\texttt{log\_reg\_imp <- glm(target$\sim$gp+oreb, data=NbaPlayers, subset=train, family=binomial)}	
	}{mod:lrimp}
\end{center}

\begin{table}[h]
	\centering
	\begin{tabular}{|| l | r | r ||} 
		\hline
		Variable & Coefficient & p-value \\
		\hline
		\hline
		intercept & -3.0111 & $<$ 0.0001 \\
		GP & 0.0488 & $<$ 0.0001 \\
		OREB & 0.6886 & $<$ 0.0001 \\	
		\hline
	\end{tabular}
	\caption{Estimated coefficients' value and corresponding p-value obtained using logistic regression with the most important regressors given by \Mod~\ref{mod:lrimp}.}
	\label{table:LRImpSum}
\end{table}

We observed that there is no substantial difference in performance. When utilizing all variables, the misclassification error rate (MER) stands at $30\%$ for model A, while it slightly increases to $32.5\%$ in the second scenario. Moreover, the AIC coefficient shifts from 1050.5 to 1052.2, which can be considered nearly unchanged.

\Tab~\ref{table:ConfMat} shows the confusion matrices for the two models. In general, both models tend to perform slightly better in predicting the ``yes'' class.

\begin{table}[h]
	\begin{subtable}{0.4\textwidth}
		\centering
		\begin{tabular}{|| cr | cc ||}    
			\hline
			\multicolumn{2}{|c|}{Model A} 
			& No & Yes \\
			\hline
			\hline
			& No & 66 & 44 \\
			& Yes & 52 & 158 \\
			\hline
		\end{tabular}
		\caption{}
		\label{table:ConfMatModA}
	\end{subtable}
	\hfill
	\begin{subtable}{0.4\textwidth}
		\centering
		\begin{tabular}{|| cr | cc ||}    
			\hline
			\multicolumn{2}{|c|}{Model B} 
			& $No$ & $Yes$ \\
			\hline
			\hline
			& No & 65 & 51 \\
			& Yes & 53 & 151 \\
			\hline
		\end{tabular}
		\caption{}
		\label{table:ConfMatModA}
	\end{subtable}
	\caption{Classification models' performance. (a) Confusion matrix of model A. (b) Confusion matrix of model B.}
	\label{table:ConfMat}
\end{table}


Another comparison made between the two models concerns the predicted probabilities, as values close to 0.5 suggest high uncertainty of the model.

To visualize this, we constructed histograms for each model. \Fig~\ref{fig:ProbPredYesA} represents the distribution of predicted values for true \textit{yes} samples in model A, while \Fig~\ref{fig:ProbPredYesB} shows the corresponding distribution for model B. Additionally, \Fig~\ref{fig:ProbPredNoA} displays the histogram of predicted values for true \textit{no} samples in model A, and \Fig~\ref{fig:ProbPredNoB} shows the same for model B.

From this perspective, it seems that model B demonstrates slightly higher confidence in its \textit{yes} predictions. However, overall, both models exhibit similar behavior.

\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{ImageFiles/Classification/LogReg/probability_pred_all_yes}
		\caption{}
		\label{fig:ProbPredYesA}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{ImageFiles/Classification/LogReg/probability_pred_all_no}
		\caption{}
		\label{fig:ProbPredNoA}
	\end{subfigure}
	\caption{Distribution of the probabilities predicted by model A. (a) Probabilities predicted on true \textit{yes} samples. (b) Probabilities predicted on true \textit{no} samples.}
	\label{fig:ProbPredA}
\end{figure}

\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{ImageFiles/Classification/LogReg/probability_pred_imp_yes}
		\caption{}
		\label{fig:ProbPredYesB}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{ImageFiles/Classification/LogReg/probability_pred_imp_no}
		\caption{}
		\label{fig:ProbPredNoB}
	\end{subfigure}
	\caption{Distribution of the probabilities predicted by model B. (a) Probabilities predicted on true \textit{yes} samples. (b) Probabilities predicted on true \textit{no} samples.}
	\label{fig:ProbPredB}
\end{figure}

The model fitted using only the important variables shows the same performance as the first one, but it uses less variables, making it a better choice.

\vspace{0.2cm}
\noindent
\textbf{Subset selection}

After the initial analysis, we attempted variable selection using subset selection algorithms, specifically the forward stepwise and backward stepwise methods were applied. 

The results obtained are presented in \Tab~\ref{table:LRAllSum}. The forward stepwise method resulted in a model that includes all the variables, like model A. Therefore, its analysis is identical to the previous one.

In contrast, the backward stepwise method resulted in a model with 11 variables, but 7 of them are not significantly different from zero with respect to $\alpha = 0.01$. The model is shown in \Tab~\ref{table:LRBSSum}. The model achieved an $AIC = 1040.4$ and a MER of $30\%$. The performances are again quiet similar, but the AIC coefficient is slightly better for the backward model.

\begin{center}
	\labelText{
		\texttt{b\_log\_reg\_fit <- stepAIC(log\_reg\_all, direction="backward", trace=FALSE)}	
	}{mod:lrbs}
\end{center}

\begin{table}[h]
	\centering
	\begin{tabular}{|| l | r | r ||} 
		\hline
		Variable & Coefficient & p-value \\
		\hline
		\hline
		intercept & -3.4011 & $<$ 0.0001 \\
		GP & 0.0455 & $<$ 0.0001 \\
		MIN & -0.0771 & 0.0205 \\
		FGM & 0.2152 & 0.1575 \\
		3P MADE & 3.3156 & 0.0055 \\
		3PA & -1.1686 & 0.0079 \\
		FTM & 0.4680 & 0.0240 \\
		FT\% & 0.0123 & 0.1425 \\
		OREB & 0.7474 & 0.0018 \\
		AST & 0.3655 & 0.0045 \\
		BLK & 0.5499 & 0.0760 \\				
		TOV & -0.6577 & 0.0453 \\
		\hline
	\end{tabular}
	\caption{Estimated coefficients' value and corresponding p-value obtained applying forward stepwise selection to a logistic regression model, given by \Mod~\ref{mod:lrbs}.}
	\label{table:LRBSSum}
\end{table}

After removing the non-significant variables from the backward model, we obtained the same model as model B. 

This confirms our results, suggesting that ``GP'' and ``OREB'' are the most important variables to explain the target using logistic regression. 

\vspace{0.2cm}
\noindent
\textbf{Conclusions and observations}

In general, regardless of the variables included, logistic regression is able to achieve an accuracy around $70\%$. Therefore, it may be better to explore other classification methods to potentially improve the model's performances.

However, the logistic regression model obtained suggests that a good trade-off between model simplicity and performances can be achieved with two variables. In particular:

\begin{itemize}
	\item The inclusion of ``GP'' was expected, as it makes sense that players who play more games will have a longer career. The second feature, on the other hand, is not really intuitive and revealed a new characteristic about the data.
	\item In general, all models perform better at predicting the ``yes'' class instead of the ``no'' class, but this can be traced back to the composition of the dataset, which includes more ``yes'' samples than ``no''.
\end{itemize}

The performances achieved with logistic regression are limited, therefore other classification techniques will be explored in the following sections.
