\subsection{Logistic regression}

As first approach we tried to model our target using logistic regression.

First of all we fitted a model using all the features, that we will name model A, and the result is displayed in \Tab~\ref{table:LRAllSum}. The model suggested that many variables are not significantly different from zero, with respect to an $\alpha = 0.01$. Therefore we removed those variables and fitted the model again getting the one in \Tab~\ref{table:LRImpSum}, that we will name model B.

\begin{table}[H]
	\begin{subtable}[h]{0.3\textwidth}
		\centering
		\begin{tabular}{|| l | r | r ||} 
			\hline
			\multicolumn{3}{|c|}{Equation} \\
			\hline
			Variable & Coefficient & p-value \\
			\hline
			intercept & -3.8885 & 0.0068 \\
			GP & 0.0449 & $<$ 0.0001 \\
			MIN & -0.0593 & 0.1420 \\
			FGM & 0.0997 & 0.8668 \\
			FGA & 0.0183 & 0.9469 \\
			FG\% & 0.0187 & 0.4721 \\
			3P MADE & 2.8171 & 0.0312 \\
			3PA & -0.9840 & 0.0473 \\
			3P\% & 0.0039 & 0.5511 \\
			FTM & 0.9664 & 0.2235 \\
			FTA & -0.3906 & 0.5074 \\
			FT\% & 0.0067 & 0.5652 \\
			OREB & 0.8793 & 0.0017 \\
			DREB & -0.1361 & 0.4137 \\
			AST & 0.3471 & 0.0111 \\
			STL & -0.1227 & 0.7414 \\
			BLK & 0.6291 & 0.0566 \\				
			TOV & -0.5353 & 0.1164 \\		
			\hline
		\end{tabular}
		\caption{Logistic regression with all features.}
		\label{table:LRAllSum}
	\end{subtable}
	\hfill
	\begin{subtable}[h]{0.3\textwidth}
		\centering
		\begin{tabular}{|| l | r | r ||} 
			\hline
			\multicolumn{3}{|c|}{Equation} \\
			\hline
			Variable & Coefficient & p-value \\
			\hline
			intercept & -3.0111 & $<$ 0.0001 \\
			GP & 0.0488 & $<$ 0.0001 \\
			OREB & 0.6886 & $<$ 0.0001 \\	
			\hline
		\end{tabular}
		\caption{Logistic regression with most significant features.}
		\label{table:LRImpSum}
	\end{subtable}
	\caption{Logistic regression model summary.}
	\label{table:LRSum}
\end{table}

It can be observed that there is no significant change in performance. When all variables are used, the accuracy is $70\%$, compared to $67.5\%$ in the second case. Furthermore, the AIC coefficient changes from 1052.5 to 1050.2, which could be considered almost the same. 

\Tab~\ref{table:ConfMat} shows the confusion matrices for the two models. In general, both models tend to perform slightly better in predicting the ``yes'' class.

\begin{table}[H]
	\begin{subtable}[h]{0.4\textwidth}
		\centering
		\begin{tabular}{|| cr | cc ||}    
			\hline
			\multicolumn{2}{|c|}{Model A} 
			& No & Yes \\
			\hline
			& No & 66 & 44 \\
			& Yes & 52 & 158 \\
			\hline
		\end{tabular}
		\caption{}
		\label{table:ConfMatModA}
	\end{subtable}
	\hfill
	\begin{subtable}[h]{0.4\textwidth}
		\centering
		\begin{tabular}{|| cr | cc ||}    
			\hline
			\multicolumn{2}{|c|}{Model B} 
			& $No$ & $Yes$ \\
			\hline
			& No & 65 & 51 \\
			& Yes & 53 & 151 \\
			\hline
		\end{tabular}
		\caption{}
		\label{table:ConfMatModA}
	\end{subtable}
	\caption{Confusion matrices.}
	\label{table:ConfMat}
\end{table}

Another comparison made between the two models concerns the predicted probabilities, as values close to 0.5 suggest high uncertainty of the model. Histograms showing the predicted values distribution for both models are presented in \Fig~\ref{fig:ProbPred}. From this point of view, model B appears to perform better, but in general, the two models exhibit the same behavior.

\begin{figure}[H]
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{ImageFiles/Classification/LogReg/probability_pred_all}
		\caption{Probabilities predicted by model A.}
		\label{fig:ProbPredA}
	\end{subfigure}
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{ImageFiles/Classification/LogReg/probability_pred_imp}
		\caption{Probabilities predicted by model B.}
		\label{fig:ProbPredB}
	\end{subfigure}
	\caption{Probabilities predicted.}
	\label{fig:ProbPred}
\end{figure}

The model fitted using only the important variables shows the same performance as the first one, but it uses variables, making it a better choice.

\vspace{0.2cm}
\textbf{Subset selection}

After the initial analysis, we attempted variable selection using subset selection algorithms, specifically the forward stepwise and backward stepwise methods were applied. 

The results obtained are presented in \Tab~\ref{table:LRSSum}. The forward stepwise method resulted in a model that essentially includes all the variables. In contrast, the backward stepwise method resulted in a model with 11 variables, but 7 of them are not significantly different from zero with respect to $\alpha = 0.01$. The model achieved an $AIC = 1040.4$ and accuracy of $70\%$. The performances are again quiet similar, the AIC coefficient is slightly better for the backward model.

\begin{table}[H]
	\begin{subtable}[h]{0.3\textwidth}
		\centering
		\begin{tabular}{|| l | r | r ||} 
			\hline
			\multicolumn{3}{|c|}{Equation} \\
			\hline
			Variable & Coefficient & p-value \\
			\hline
			intercept & -3.8885 & 0.0068 \\
			GP & 0.0449 & $<$ 0.0001 \\
			MIN & -0.0593 & 0.1420 \\
			FGM & 0.0997 & 0.8668 \\
			FGA & 0.0183 & 0.9469 \\
			FG\% & 0.0187 & 0.4721 \\
			3P MADE & 2.8171 & 0.0312 \\
			3PA & -0.9840 & 0.0473 \\
			3P\% & 0.0039 & 0.5511 \\
			FTM & 0.9664 & 0.2235 \\
			FTA & -0.3906 & 0.5074 \\
			FT\% & 0.0067 & 0.5652 \\
			OREB & 0.8793 & 0.0017 \\
			DREB & -0.1361 & 0.4137 \\
			AST & 0.3471 & 0.0111 \\
			STL & -0.1227 & 0.7414 \\
			BLK & 0.6291 & 0.0566 \\				
			TOV & -0.5353 & 0.1164 \\		
			\hline
		\end{tabular}
		\caption{Logistic regression with forward stepwise selection.}
		\label{table:LRFSSSum}
	\end{subtable}
	\hfill
	\begin{subtable}[h]{0.3\textwidth}
		\centering
		\begin{tabular}{|| l | r | r ||} 
			\hline
			\multicolumn{3}{|c|}{Equation} \\
			\hline
			Variable & Coefficient & p-value \\
			\hline
			intercept & -3.4011 & $<$ 0.0001 \\
			GP & 0.0455 & $<$ 0.0001 \\
			MIN & -0.0771 & 0.0205 \\
			FGM & 0.2152 & 0.1575 \\
			3P MADE & 3.3156 & 0.0055 \\
			3PA & -1.1686 & 0.0079 \\
			FTM & 0.4680 & 0.0240 \\
			FT\% & 0.0123 & 0.1425 \\
			OREB & 0.7474 & 0.0018 \\
			AST & 0.3655 & 0.045 \\
			BLK & 0.5499 & 0.0760 \\				
			TOV & -0.6577 & 0.0453 \\
			\hline
		\end{tabular}
		\caption{Logistic regression with backward stepwise selection.}
		\label{table:LRBSSum}
	\end{subtable}
	\caption{Logistic regression models with subset selection.}
	\label{table:LRSSum}
\end{table}

Since the forward model includes all the variables, its analysis is identical to the first one.

After removing the non-significant variables from the backward model, we obtained the same model as in the previous analysis, which we referred to as model B. 

This confirms our results, suggesting that the model with only ``GP'' and ``OREB'' explains in a good way the target. 

\vspace{0.2cm}
\textbf{Conclusions and observations}

In general, regardless of the variables included, logistic regression is able to achieve an accuracy around $70\%$. Therefore, it may be better to explore other classification methods to potentially improve the model's performances.

However, the logistic regression model obtained suggests that a good trade-off between model simplicity and performances can be achieved with two variables. In particular:

\begin{itemize}
	\item The inclusion of ``GP'' was expected, as it makes sense that players who play more games will have a longer career. The second feature, on the other hand, is not really intuitive and revealed a new characteristic about the data.
	\item In general, all models perform better at predicting the ``yes'' class instead of the ``no'' class, but this can be traced back to the composition of the dataset, which includes more ``yes'' samples than ``no''.
\end{itemize}

The performances achieved with logistic regression are limited, therefore other classification techniques will be explored in the following sections.
