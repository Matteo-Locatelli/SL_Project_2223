\subsection*{Classification techniques}\label{appendix:ct}

\begin{table}[H]
	\centering
	\begin{tabular}{|| l | r | r ||} 
		\hline
		\multicolumn{3}{|c|}{Equation} \\
		\hline
		Variable & Coefficient & p-value \\
		\hline
		intercept & -3.8885 & 0.0068 \\
		GP & 0.0449 & $<$ 0.0001 \\
		MIN & -0.0593 & 0.1420 \\
		FGM & 0.0997 & 0.8668 \\
		FGA & 0.0183 & 0.9469 \\
		FG\% & 0.0187 & 0.4721 \\
		3P MADE & 2.8171 & 0.0312 \\
		3PA & -0.9840 & 0.0473 \\
		3P\% & 0.0039 & 0.5511 \\
		FTM & 0.9664 & 0.2235 \\
		FTA & -0.3906 & 0.5074 \\
		FT\% & 0.0067 & 0.5652 \\
		OREB & 0.8793 & 0.0017 \\
		DREB & -0.1361 & 0.4137 \\
		AST & 0.3471 & 0.0111 \\
		STL & -0.1227 & 0.7414 \\
		BLK & 0.6291 & 0.0566 \\				
		TOV & -0.5353 & 0.1164 \\		
		\hline
	\end{tabular}
	\caption{Logistic regression with all features.}
	\label{table:LRAllSum}
\end{table}

\noindent
\textbf{Logistic regression}

\begin{center}

\labelText{
		\texttt{log\_reg\_all <- glm(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, family=binomial)}	\\
	}{mod:lrall}

\labelText{
		\texttt{log\_reg\_imp <- glm(target$\sim$gp+oreb, data=NbaPlayers, subset=train, family=binomial)}	\\
	}{mod:lrimp}

\end{center}

\noindent
\textbf{Logistic regression subset selection}

\begin{center}
	
\labelText{
		\texttt{f\_log\_reg\_fit <- stepAIC(log\_reg\_all, direction="forward", trace=FALSE)}	\\
	}{mod:lrfs}

\labelText{
		\texttt{b\_log\_reg\_fit <- stepAIC(log\_reg\_all, direction="backward", trace=FALSE)}	\\
	}{mod:lrbs}

\labelText{
		\texttt{log\_reg\_final <- glm(target$\sim$gp+oreb, data=NbaPlayers, subset=train, family=binomial)}	\\
	}{mod:lrfin}

\end{center}
	
\noindent
\textbf{Trees}

\begin{center}

\labelText{
		\texttt{tmc <- tree(target$\sim$.-target\_5yrs, NbaPlayers, subset=train, split="gini")}
	}{mod:treecom}

--- figure 8
\labelText{
		\texttt{tree\_cv <- cv.tree(tree\_model, FUN=prune.misclass)}
	}{mod:labpr}

\texttt{prune1 <- prune.misclass(tree\_model, best=best1, k=k1)}

\texttt{prune2 <- prune.misclass(tree\_model, best=best2, k=k2)}

\texttt{prune3 <- prune.misclass(tree\_model, best=best3)} --- k=-Inf

\texttt{prune4 <- prune.misclass(tree\_model, best=best4, k=k4)}

\end{center}

\noindent
\textbf{Ensemble methods}

\begin{center}

\labelText{
		\texttt{bagg\_model <- randomForest(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, mtry=ncol(NbaPlayers)-2, ntree=500, importance=TRUE, replace=TRUE)}	\\
	}{mod:bag}

\texttt{forest\_model <- randomForest(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, mtry=floor(sqrt(ncol(NbaPlayers)-2)), ntree=500, importance=TRUE)}

\labelText{
		\texttt{best\_rf <- randomForest(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, mtry=best\_m, ntree=500, importance = TRUE)}	\\
	}{mod:rf}

\labelText{
		\texttt{boost\_model <- gbm(target\_5yrs$\sim$.-target, data=NbaPlayers[train,], distribution="bernoulli", n.trees=ntrees, cv.folds=10, interaction.depth=4,	 shrinkage=0.001, verbose=F)}	\\
	}{mod:boost}

\end{center}
	
\noindent
\textbf{KNN}

\begin{center}

\labelText{
		\texttt{knn\_fit <- knn(data\_train, data\_test, label\_train, k=3)}	\\
	}{mod:knn}

\end{center}
	
\noindent
\textbf{SVM}

\begin{center}

\labelText{
		\texttt{svm\_cross <- tune (svm, y$\sim$., data=dataframe\_train, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))}	\\
	}{mod:svmcross}

\labelText{
		\texttt{svm\_rad\_tune <- tune(svm, y$\sim$., data=dataframe\_train, kernel="radial", ranges=list(cost=c(0.01, 0.1, 1, 10, 100), gamma=c(0.5, 1, 2, 3, 4)))}	\\
	}{mod:svmrad}

\labelText{	
		\texttt{svm\_pol\_tune <- tune(svm, y$\sim$., data=dataframe\_train, kernel="polynomial", ranges=list(cost=c(0.01, 0.1, 1, 10, 100), degree=c(0.5, 1, 2, 3, 4)))}	\\
	}{mod:svmpol}

\end{center}