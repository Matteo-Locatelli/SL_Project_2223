\subsection*{Classification techniques}\label{appendix:ct}

\begin{table}[H]
	\centering
	\begin{tabular}{|| l | r | r ||} 
		\hline
		\multicolumn{3}{|c|}{Equation} \\
		\hline
		Variable & Coefficient & p-value \\
		\hline
		intercept & -3.8885 & 0.0068 \\
		GP & 0.0449 & $<$ 0.0001 \\
		MIN & -0.0593 & 0.1420 \\
		FGM & 0.0997 & 0.8668 \\
		FGA & 0.0183 & 0.9469 \\
		FG\% & 0.0187 & 0.4721 \\
		3P MADE & 2.8171 & 0.0312 \\
		3PA & -0.9840 & 0.0473 \\
		3P\% & 0.0039 & 0.5511 \\
		FTM & 0.9664 & 0.2235 \\
		FTA & -0.3906 & 0.5074 \\
		FT\% & 0.0067 & 0.5652 \\
		OREB & 0.8793 & 0.0017 \\
		DREB & -0.1361 & 0.4137 \\
		AST & 0.3471 & 0.0111 \\
		STL & -0.1227 & 0.7414 \\
		BLK & 0.6291 & 0.0566 \\				
		TOV & -0.5353 & 0.1164 \\		
		\hline
	\end{tabular}
	\caption{Logistic regression with all features.}
	\label{table:LRAllSum}
\end{table}

\noindent
\textbf{Logistic regression}

\begin{center}
\texttt{NbaPlayers <- subset(NbaPlayers, select=c(-pts, -reb))}

\texttt{NbaPlayers\$target <- factor(ifelse(NbaPlayers\$target\_5yrs == 0 , " No " , " Yes "))}

\texttt{train <- sample(1:nrow(NbaPlayers), floor(nrow(NbaPlayers)*0.75))}

\texttt{log\_reg\_all <- glm(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, family=binomial)}

\texttt{log\_reg\_imp <- glm(target$\sim$gp+oreb, data=NbaPlayers, subset=train, family=binomial)}
\end{center}

\noindent
\textbf{Logistic regression subset selection}

\begin{center}
\texttt{NbaPlayers <- subset(NbaPlayers, select=c(-pts, -reb))}

\texttt{NbaPlayers\$target <- factor(ifelse(NbaPlayers\$target\_5yrs == 0 , " No " , " Yes "))}

\texttt{train <- sample(1:nrow(NbaPlayers), floor(nrow(NbaPlayers)*0.75))}

\texttt{log\_reg\_all <- glm(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, family=binomial)}

\texttt{f\_log\_reg\_fit <- stepAIC(log\_reg\_all, direction="forward", trace=FALSE)}

\texttt{b\_log\_reg\_fit <- stepAIC(log\_reg\_all, direction="backward", trace=FALSE)}

\texttt{log\_reg\_final <- glm(target$\sim$gp+oreb, data=NbaPlayers, subset=train, family=binomial)}
\end{center}
	
\noindent
\textbf{Trees}

\begin{center}
\texttt{NbaPlayers <- subset(NbaPlayers, select=c(-pts, -reb))}

\texttt{NbaPlayers\$target <- factor(ifelse(NbaPlayers\$target\_5yrs == 0 , " No " , " Yes "))}

\texttt{train <- sample(1:nrow(NbaPlayers), floor(nrow(NbaPlayers)*0.75))}

\texttt{tmc <- tree(target$\sim$.-target\_5yrs, NbaPlayers, subset=train, split="gini")}

\texttt{tree\_cv <- cv.tree(tree\_model, FUN=prune.misclass)} --- figure 8

\texttt{best1 = min(tree\_cv\$size[tree\_cv\$dev == min(tree\_cv\$dev)])} --- 9a

\texttt{k1 = min(tree\_cv\$k[tree\_cv\$size == best1])}

\texttt{prune1 <- prune.misclass(tree\_model, best=best1, k=k1)}

\texttt{best2 = tree\_cv\$size[tree\_cv\$size == 6]} --- 9b

\texttt{k2 = min(tree\_cv\$k[tree\_cv\$size == best2])}

\texttt{prune2 <- prune.misclass(tree\_model, best=best2, k=k2)}

\texttt{best3 = tree\_cv\$size[tree\_cv\$size == 7]} --- 9c

\texttt{k3 = min(tree\_cv\$k[tree\_cv\$size == best3])}

\texttt{prune3 <- prune.misclass(tree\_model, best=best3)} --- k=-Inf

\texttt{best4 = min(tree\_cv\$size[tree\_cv\$size == 2])} --- 9d

\texttt{k4 = min(tree\_cv\$k[tree\_cv\$size == best4])}

\texttt{prune4 <- prune.misclass(tree\_model, best=best4, k=k4)}
\end{center}

\noindent
\textbf{Ensemble methods}

\begin{center}
\texttt{NbaPlayers <- subset(NbaPlayers, select=c(-pts, -reb))}

\texttt{NbaPlayers\$target <- factor(ifelse(NbaPlayers\$target\_5yrs == 0, " No " , " Yes " ))}

\texttt{train <- sample(1:nrow(NbaPlayers), floor(nrow(NbaPlayers)*0.75))}

\texttt{bagg\_model <- randomForest(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, mtry=ncol(NbaPlayers)-2, ntree=500, importance=TRUE, replace=TRUE)}

\texttt{forest\_model <- randomForest(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, mtry=floor(sqrt(ncol(NbaPlayers)-2)), ntree=500, importance=TRUE)}

\texttt{best\_m = which.min(test.err.rate)}

\texttt{best\_rf <- randomForest(target$\sim$.-target\_5yrs, data=NbaPlayers, subset=train, mtry=best\_m, ntree=500, importance = TRUE)}

\texttt{ntrees = 5000}

\texttt{boost\_model <- gbm(target\_5yrs$\sim$.-target, data=NbaPlayers[train,], distribution="bernoulli", n.trees=ntrees, cv.folds=10, interaction.depth=4, shrinkage=0.001, verbose=F)}
\end{center}
	
\noindent
\textbf{KNN}

\begin{center}
\texttt{NbaPlayers <- subset(NbaPlayers, select=c(-pts, -reb))}

\texttt{NbaPlayers\$target <- factor(ifelse(NbaPlayers\$target\_5yrs == 0, " No " , " Yes "))}

\texttt{train <- sample(1:nrow(NbaPlayers), floor(nrow(NbaPlayers)*0.75))}

\texttt{matrix <- NbaPlayers[train,]}

\texttt{data\_train <- subset(NbaPlayers[train,], select=c(-target\_5yrs, -target))} 

\texttt{data\_test <- subset(NbaPlayers[-train,], select=c(-target\_5yrs, -target))}

\texttt{label\_train <- NbaPlayers\$target[train]}

\texttt{knn\_fit <- knn(data\_train, data\_test, label\_train, k=3)}
\end{center}
	
\noindent
\textbf{SVM}

\begin{center}
\texttt{NbaPlayers <- subset(NbaPlayers, select=c(-pts, -reb))}

\texttt{train <- sample(1:nrow(NbaPlayers), floor(nrow(NbaPlayers)*0.75))}

\texttt{x\_train = subset(NbaPlayers[train,], select=c(-target\_5yrs))}

\texttt{y\_train = NbaPlayers[train,]\$target\_5yrs}

\texttt{dataframe\_train <- data.frame(x=x\_train, y=as.factor(y\_train))}

\texttt{x\_test = subset(NbaPlayers[-train,], select=c(-target\_5yrs))}

\texttt{y\_test = NbaPlayers[-train,]\$target\_5yrs}

\texttt{dataframe\_test <- data.frame(x=x\_test, y=as.factor(y\_test))}

\texttt{svm\_cross <- tune (svm, y$\sim$., data=dataframe\_train, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))}

\texttt{svm\_rad\_tune <- tune(svm, y$\sim$., data=dataframe\_train, kernel="radial", ranges=list(cost=c(0.01, 0.1, 1, 10, 100), gamma=c(0.5, 1, 2, 3, 4)))}

\texttt{svm\_pol\_tune <- tune(svm, y$\sim$., data=dataframe\_train, kernel="polynomial", ranges=list(cost=c(0.01, 0.1, 1, 10, 100), degree=c(0.5, 1, 2, 3, 4)))}
\end{center}